{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhoo\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning:\n",
      "\n",
      "This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.plotly as py\n",
    "from geopy.geocoders import Nominatim\n",
    "import geopy.distance as gdist\n",
    "import numpy as np\n",
    "import collections\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import neighbors\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.colors import Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses adapted version of function accessed online to get rid of NAN values an replace with -\n",
    "# https://quant.stackexchange.com/questions/31094/removing-nan-values-in-python-quantopian\n",
    "# Written by stack exchange user: Andr√© Christoffer Andersen \n",
    "# all other numeric float values have commas stripped\n",
    "def clean_data(df, cols):\n",
    "    for i in cols:\n",
    "        df[i] = df[i].apply(lambda x: \n",
    "                            np.nan if x=='-' else float(x.replace(',', '')))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the socio-economic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning:\n",
      "\n",
      "\n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wealth = pd.read_csv('socioeconomic.csv')\n",
    "wealth = wealth[wealth['State']== 'VIC']\n",
    "wealth['Suburb'] = wealth['Suburb'].apply(lambda x: x.upper())\n",
    "# uses regex to get rid of parenthesis and its contents from suburb names\n",
    "wealth['Suburb'] = wealth['Suburb'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "clean_data(wealth,['Usual Resident Population'])\n",
    "wealth = wealth.ix[:, ['Suburb', 'Usual Resident Population', 'Score', 'State']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_suburbs(df, filters):\n",
    "    rem = []\n",
    "    for val, row in df.iterrows():\n",
    "        if (row['Suburb'] not in filters):\n",
    "            rem.append(val)\n",
    "    df.drop(rem, inplace = True)\n",
    "    df.index = range(len(df.index))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(df):\n",
    "    #restricts dataset to contain only victorian data\n",
    "    df = df[df['State']== 'VIC']\n",
    "    #converts subrubs into uppercase\n",
    "    df['Suburb'] = df['Suburb'].apply(lambda x: x.upper())\n",
    "    #uses regex to get rid of parenthesis and its contents\n",
    "    df['Suburb'] = df['Suburb'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "    #gets rid of suburbs that are not in both datasets\n",
    "    com_set = set(wealth['Suburb']).intersection(set(df['Suburb']))\n",
    "    df = filter_suburbs(df, com_set)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "C:\\Users\\abhoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "C:\\Users\\abhoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resources = pd.read_csv('eco_resources.csv')\n",
    "resources = clean_up(resources)\n",
    "edu = pd.read_csv('education.csv')\n",
    "edu = clean_up(edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'common' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c620f9052a67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#integrates the minimum resources score and minimum education and income score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcommon_wealth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwealth\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Suburb'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Suburb'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mwealth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_suburbs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwealth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0medu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_suburbs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwealth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwealth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'min education and income score'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0medu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Min Score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'common' is not defined"
     ]
    }
   ],
   "source": [
    "#integrates the minimum resources score and minimum education and income score\n",
    "common_wealth = set(wealth['Suburb']).intersection(set(edu['Suburb']))\n",
    "wealth = filter_suburbs(wealth, common)\n",
    "edu = filter_suburbs(edu, common)\n",
    "wealth.insert(loc = wealth.shape[1], column = 'min education and income score', value = edu['Min Score'])\n",
    "wealth.insert(loc = wealth.shape[1], column = 'min resource score', value = resources['Min score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the crime dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime = pd.read_csv('crime.csv', encoding  = 'ISO-8859-1')\n",
    "suburbs = {}\n",
    "discard = []\n",
    "prev = ''\n",
    "new_crime = pd.DataFrame({'Year': crime['Year ending September'],\n",
    "                         'Suburb': crime['Suburb/Town Name'],\n",
    "                         'Incidents': crime['Incidents Recorded']})\n",
    "new_crime = new_crime[new_crime['Year'] == 2016]\n",
    "new_crime = clean_data(new_crime, ['Incidents'])\n",
    "new_crime = new_crime.groupby(['Year', 'Suburb']).sum()\n",
    "new_crime.to_csv('newcrime1.csv', index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_crime = pd.read_csv('newcrime1.csv')\n",
    "common = set(wealth['Suburb']).intersection(set(n_crime['Suburb']))\n",
    "#ensures both datasets only contain the same suburbs \n",
    "n_crime = filter_suburbs(n_crime, common)\n",
    "wealth = filter_suburbs(wealth, common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated = pd.DataFrame({'Suburb': n_crime['Suburb'],\n",
    "                       'Socio-economic Ranking': wealth['Score'],\n",
    "                       'Crime Incidents': n_crime['Incidents'],\n",
    "                    'Population': wealth['Usual Resident Population'],\n",
    "                          'Minimum Resource Score': wealth['min resource score'],\n",
    "                          'Minimum education and income score': wealth['min education and income score']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding longitude and latitude of suburbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_lat = {}\n",
    "error_suburbs = []\n",
    "rem = []\n",
    "geolocator = Nominatim(timeout = None)\n",
    "for y in integrated['suburb']:\n",
    "    location = geolocator.geocode(y+ ' ' + \"VIC\")\n",
    "    print (y)\n",
    "    if location == None:\n",
    "        error_suburbs.append(y)\n",
    "        continue\n",
    "    else:\n",
    "        long_lat[y] = (location.longitude, location.latitude)\n",
    "integrated = filter_suburbs(integrated, error_suburbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson Correlation and Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the code from Workshop ipynb: Example of Pearson Correlation and mutual information.ipynb\n",
    "def entropy(T):\n",
    "    H=0\n",
    "    N=sum(T)\n",
    "    for i in range(len(T)):\n",
    "       if T[i]>0: H+=-T[i]/N*np.log2(T[i]/N)\n",
    "    return H\n",
    "def mutualInfo(x,y):\n",
    "    x=(np.asarray(x)).astype(int)\n",
    "    y=(np.asarray(y)).astype(int)\n",
    "    assert(len(x)==len(y))\n",
    "    \n",
    "    nx=max(x)\n",
    "    Tx=np.zeros(nx+1)\n",
    "    for i in range(len(x)):\n",
    "        Tx[x[i]]+=1.0\n",
    "    Hx=entropy(Tx)\n",
    "\n",
    "    ny=max(y)\n",
    "    Ty=np.zeros(ny+1)\n",
    "    for i in range(len(y)):\n",
    "        Ty[y[i]]+=1.0\n",
    "    Hy=entropy(Ty)\n",
    "    \n",
    "    T=np.zeros((nx+1,ny+1))\n",
    "    for i in range(len(x)):\n",
    "        T[x[i],y[i]]+=1.0\n",
    "    Hxy=0\n",
    "    for i in range(nx+1):\n",
    "        for j in range(ny+1):\n",
    "            if T[i,j]>0:  Hxy+=-T[i,j]/len(x)*np.log2(T[i,j]/len(x))\n",
    "    \n",
    "    Hxgy=Hxy-Hy\n",
    "    Hygx=Hxy-Hx\n",
    "    minH=min(Hx,Hy)\n",
    "    \n",
    "    return {'Hx':Hx,'Hy':Hy,'Hx|y': Hxgy , 'Hy|x': Hygx ,'MI':Hx+Hy-Hxy,'NMI':(Hx+Hy-Hxy)/minH} \n",
    "\n",
    "print(\"Pearson r is \",integrated['Crime Incidents'].corr(integrated['Socio-economic Ranking']))\n",
    "result=mutualInfo(integrated.loc[:,'Socio-economic Ranking'],integrated.loc[:,'Crime Incidents'])\n",
    "print(\"Entropies and mutual information are\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-NN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a dataframe column called high crime where crime incidents above the 75th percentile\n",
    "#range of data are considered high (represented by 1) and other lows (0)\n",
    "high_crime = []\n",
    "for val, row in integrated.iterrows():\n",
    "    if row['Crime Incidents'] >= 150:#np.percentile(n['crime'], 75):\n",
    "        high_crime.append(1)\n",
    "    else:\n",
    "        high_crime.append(0)\n",
    "integrated.insert(loc = integrated.shape[1], column = 'high_crime', value = high_crime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate accuracy score using K-NN algorithm from Workshop 8\n",
    "\n",
    "##get just the features\n",
    "data=integrated[[ 'Minimum education and income score', 'Minimum Resource Score', 'Population', 'Socio-economic Ranking']]\n",
    "\n",
    "\n",
    "##get just the class labels\n",
    "classlabel=integrated['high_crime']\n",
    "\n",
    "##randomly select 66% of the instances to be training and the rest to be testing\n",
    "# random state parameter ensures that everytime code is run, same split occurs\n",
    "X_train, X_test, y_train, y_test = train_test_split(data,classlabel, train_size=0.66, random_state=42)\n",
    "\n",
    "#normalise the data to have 0 mean and unit variance using the library functions.  This will help for later\n",
    "#computation of distances between instances\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train) #each feature column between 0 and 1\n",
    "X_train=scaler.transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred=knn.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(mat.T, square = True, annot = True, fmt = 'd',\n",
    "           cbar = False)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.savefig('falsepos.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier detection and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier analysis done through adapting code from online\n",
    "# https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-dataframe\n",
    "# written by Stack Overflow user: user2708149 \n",
    "def noise_outlier(df_col):\n",
    "    median_value = df_col.median()\n",
    "    smaller_v = df_col[df_col < median_value]\n",
    "    larger_v = df_col[df_col > median_value]\n",
    "    iqr = larger_v.median() - smaller_v.median()\n",
    "    noise_up = larger_v.median() + (iqr*1.5)\n",
    "    noise_low = smaller_v.median() - (iqr*1.5)\n",
    "    max_upp  =  larger_v.median() + (iqr*3)\n",
    "    max_low = smaller_v.median() - (iqr*3)\n",
    "    noise = sum(((s < noise_low) & (s > max_low))) + sum(((s > noise_up) & (s < max_up)))\n",
    "    outliers = sum( s > max_up) + sum(s < max_low)\n",
    "    return noise, outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates boxplot to show outliers\n",
    "# varied the attributes as required\n",
    "n['crime'].plot.box()\n",
    "plt.title('Crime Outliers')\n",
    "plt.savefig('crime outliers.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plot with Linear regression line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created scatterplot with linear regression using\n",
    "# accessed from https://github.com/OpenGenus/quark/blob/master/code/code/artificial_intelligence/src/Linear_Regression/linear_regression.py\n",
    "# Written by github user: AdiChat\n",
    "def estimate_coef(x, y):\n",
    "    # number of observations/points\n",
    "    n = np.size(x)\n",
    "\n",
    "    # mean of x and y vector\n",
    "    m_x, m_y = np.mean(x), np.mean(y)\n",
    "\n",
    "    # calculating cross-deviation and deviation about x\n",
    "    SS_xy = np.sum((x - m_x) * (y - m_y))\n",
    "    SS_xx = np.sum(x*x - m_x*m_x)\n",
    "\n",
    "    # calculating regression coefficients\n",
    "    b_1 = SS_xy / SS_xx\n",
    "    b_0 = m_y - b_1 * m_x\n",
    "\n",
    "    return (b_0, b_1)\n",
    "\n",
    "def plot_regression_line(xs, ys):\n",
    "    # dev stands for deviation\n",
    "    dev = estimate_coef(xs, ys)\n",
    "\n",
    "    y_pred = []\n",
    "    for x in xs:\n",
    "        y_pred.append(dev[0] + dev[1] * x)\n",
    "\n",
    "    # plotting the regression line\n",
    "    plt.plot(xs, y_pred, color = \"g\")\n",
    "new = integrated[integrated['crime']<400]\n",
    "xarr = new['crime']\n",
    "yarr = new['socio-economic status']\n",
    "\n",
    "# Setting points as numpy arrays.\n",
    "# It is more convenient this way for further process.\n",
    "x = np.array(xarr)\n",
    "y = np.array(yarr)\n",
    "\n",
    "# Plotting points.\n",
    "plt.scatter(x, y)\n",
    "\n",
    "plot_regression_line(x, y)\n",
    "plt.xlabel('Number of Crime Incidents')\n",
    "plt.ylabel('Socio-economic ranking')\n",
    "plt.tight_layout()\n",
    "plt.savefig('scatter_regression_sm.png', dpi = 1080)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots the parallel coordinates of the suburbs\n",
    "# can set the legend also to suburbs but too many suburbs so hard to read\n",
    "# normalises the data\n",
    "integrated['Crime Incidents'] = (integrated['Crime Incidents'] -integrated['Crime Incidents'].min())/(integrated['Crime Incidents'].max()-integrated['Crime Incidents'].min())\n",
    "integrated['Socio-economic Ranking'] = (integrated['Socio-economic Ranking'] -integrated['Socio-economic Ranking'].min())/(integrated['Socio-economic Ranking'].max()-integrated['Socio-economic Ranking'].min())\n",
    "integrated['Population'] = (integrated['Population'] -integrated['Population'].min())/(integrated['Population'].max()-integrated['Population'].min())\n",
    "integrated['Minimum Resource Score'] = (integrated['Minimum Resource Score'] -integrated['Minimum Resource Score'].min())/(integrated['Minimum Resource Score'].max()-integrated['Minimum Resource Score'].min())\n",
    "integrated['Minimum education and income score'] = (integrated['Minimum education and income score'] -integrated['Minimum education and income score'].min())/(integrated['Minimum education and income score'].max()-integrated['Minimum education and income score'].min())#the average for every day of the week is computed\n",
    "\n",
    "n['name'] = ''\n",
    "\n",
    "#plot the data using parallel coordinates\n",
    "parallel_coordinates(n[['Crime Incidents','Socio-economic Ranking','Population','Minimum Resource Score','Minimum education and income score', 'name']],'name')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a geographic heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts a multiple dimension shapefile to 2D and makes sure that format\n",
    "# is consistent with existing dataset (uppercase suburbs with no parenthesis)\n",
    "unwanted = [\"NSW\", \"ACT\", \"NT\", \"QLD\", \"SA\", \"WA\"]\n",
    "sub = gpd.read_file(\"./SSC_2011_AUST.shp\")\n",
    "sub['SSC_NAME'] = sub['SSC_NAME'].apply(lambda x: x.upper())\n",
    "\n",
    "for i in unwanted:\n",
    "    sub = sub[sub['SSC_NAME'].str.contains(i) == False]\n",
    "\n",
    "commons = set(integrated['Suburb']).intersection(set(sub['SSC_NAME']))\n",
    "sub = filter_suburbs(sub, commons)\n",
    "\n",
    "v = shapefile.geometry.apply(lambda x: True if x != None else False)\n",
    "shapefile = shapefile[v]\n",
    "shapefile.to_file(output_filename, driver = 'ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from https://github.com/jamalmoir/notebook_playground/blob/master/uk_2015_houseprice.ipynb\n",
    "# Github user name: jamalmoir\n",
    "# building a geographic heatmap of victoria\n",
    "\n",
    "#builds the basemap, which is Victoria in this case\n",
    "fig, ax = plt.subplots(figsize=(100,100))\n",
    "m = Basemap(projection='merc',llcrnrlat=-39.3866,urcrnrlat=-33.9813,\\\n",
    "            llcrnrlon=139.1832,urcrnrlon=150.9715,resolution='h')\n",
    "m.drawmapboundary(fill_color='#46bcec')\n",
    "m.fillcontinents(color='#f2f2f2',lake_color='#46bcec')\n",
    "m.drawcoastlines()\n",
    "\n",
    "# reads shapefile and merges with integrated dataset\n",
    "m.readshapefile('out', 'area')\n",
    "df_poly = pd.DataFrame({\n",
    "        'shapes': [Polygon(np.array(shape), True) for shape in m.area],\n",
    "        'area': [area['SSC_NAME'] for area in m.area_info]\n",
    "    })\n",
    "df_poly = df_poly.merge(integrated, on = 'area', how='inner')\n",
    "\n",
    "# plots the heatmap\n",
    "cmap = plt.get_cmap('PuRd')   \n",
    "pc = PatchCollection(df_poly.shapes, zorder=2)\n",
    "norm = Normalize()\n",
    "\n",
    "pc.set_facecolor(cmap(norm(df_poly['crime'].fillna(0).values)))\n",
    "ax.add_collection(pc)\n",
    "\n",
    "mapper = matplotlib.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "mapper.set_array(df_poly['crime'])\n",
    "plt.colorbar(mapper, shrink=0.4)\n",
    "plt.title('Socio-economic ranking hotspots')\n",
    "plt.savefig('crime_not_norm.png')\n",
    "m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
